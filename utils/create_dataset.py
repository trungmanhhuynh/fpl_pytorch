#! /usr/bin/env python
# -*- coding: utf-8 -*-
# vim:fenc=utf-8
#
# Copyright Â© 2018 Takuma Yagi <tyagi@iis.u-tokyo.ac.jp>
#
# Distributed under terms of the MIT license.

from __future__ import print_function
from __future__ import division
from six.moves import range

import os
import argparse
import json
import time
import joblib
import datetime

import numpy as np


def read_sfmlearn(ego_path, flip):
    """
    Right-hand coordinate (following SfMLearn paper).
    X: Pitch, Y: Yaw, Z: Roll
    """
    ego_dict = {}
    with open(ego_path, "r") as f:
        for line in f:
            strings = line.strip("\r\n").split(",")
            key = strings[0]
            vx, vy, vz, rx, ry, rz = list(map(lambda x:float(x), strings[1:]))
            if flip:
                ego_dict[key] = np.array([rx, -ry, -rz, -vx, vy, vz])
            else:
                ego_dict[key] = np.array([rx, ry, rz, vx, vy, vz])
    return ego_dict


def read_gridflow(ego_path, flip):
    ego_dict = {}
    with open(ego_path) as f:
        for line in f:
            strings = line.strip("\r\n").split(",")
            key = strings[0]
            grid_flow = np.array(list(map(float, strings[1:])))
            if flip:
                grid_flow[range(0, len(grid_flow), 2)] *= -1
            ego_dict[key] = grid_flow
    return ego_dict


def read_vid_list(indir_list):
    vid_list, nb_image_list = [], []
    blacklist = {}
    with open(indir_list) as f:
        for line in f:
            strs = line.strip("\n").split(",")
            if strs[0].startswith("#"):
                continue
            vid_list.append(strs[0])
            nb_image_list.append(int(strs[1]))
            if len(strs) > 2:
                blacklist[strs[0]] = strs[2:]

    return vid_list, nb_image_list, blacklist


if __name__ == "__main__":
    """
    Create dataset for training
    """

    # Define input argurments
    parser = argparse.ArgumentParser()
    parser.add_argument('indir_list', type=str, help ="List of video used to extract data")
    parser.add_argument('--traj_length', type=int, default=30,help = "trajectory length")
    parser.add_argument('--traj_skip', type=int, default=2, help = "number of frames are skipped in training data")
    parser.add_argument('--traj_skip_test', type=int, default=5, help = "number of frames are skipped in testing data")
    parser.add_argument('--nb_splits', type=int, default=3, help = "number of splits")
    parser.add_argument('--width', type=int, default=1280, help ="width of video frame")
    parser.add_argument('--height', type=int, default=960, help ="height of video frame")
    parser.add_argument('--skip', type=int, default=1, help ="It is not used")
    parser.add_argument('--seed', type=int, default=1701, help = "seed to generate random splits")
    parser.add_argument('--debug', action="store_true", default= False)
    parser.add_argument('--no_flip', action="store_true")
    parser.add_argument('--ego_type', type=str, default="sfm")
    args = parser.parse_args()
    data_dir = "data/"                    # Directory of raw inputs
    #h, w = args.height, args.width       # It is not actually used.

    start = time.time()                    # Start timer

    # Define output file - pre-processed data 
    date = datetime.datetime.now()
    out_fn = "dataset/{}_{}_{}_sp{}_{}_{}.joblib".format(
        os.path.splitext(os.path.basename(args.indir_list))[0], date.strftime('%y%m%d_%H%M%S'),
        args.traj_length, args.nb_splits,
        args.traj_skip, args.traj_skip_test)

    # Extract: 
    #    vid_list - list of video names
    #    nb_image_list - list of number of image frames for each video
    #    blacklist  - 
    vid_list, nb_image_list, blacklist = read_vid_list(args.indir_list)
    print("Number of videos: {}".format(len(vid_list)))

    # Shuffle video names using seed. With same seed, the resulted shuffed_ids
    # are the same for every run, so change the seed number to have different shuffle.
    shuffled_ids = np.copy(vid_list)
    np.random.seed(args.seed)
    np.random.shuffle(shuffled_ids)


    # Split video files into different splits, defined by nb_slits,
    ratio = 1 / args.nb_splits
    nb_videos = len(vid_list)
    split_dict = {}
    print("Shuffled videos in each split:")
    for sp, st in enumerate(np.arange(0, 1, ratio)):
        print(shuffled_ids[int(nb_videos*st):int(nb_videos*(st+ratio))])
        for vid in shuffled_ids[int(nb_videos*st):int(nb_videos*(st+ratio))]:
            split_dict[vid] = sp

    video_ids, frames, person_ids, trajectories, poses, splits, \
        turn_mags, trans_mags, pids = [], [], [], [], [], [], [], [], []
    start_dict, traj_dict, pose_dict = {}, {}, {}

    total_frames = 0                  # Intialize the total number of frames across all video sequences.              
    egomotion_dict = {}
    nb_trajs = 0
    nb_traj_list = [0 for _ in range(args.nb_splits)]


    for video_id, nb_images in zip(vid_list, nb_image_list):

        total_frames += nb_images      # update the total number of frames.

        # Read trajectories files
        trajectory_path = os.path.join(data_dir, "trajectories/{}_trajectories_dynamic.json".format(video_id))
        with open(trajectory_path, "r") as f:
            trajectory_dict = json.load(f)

        #sfm is the default ego type given in the author's github.
        #sfm: learing structure from motions. sfm defines 6 features [rx, ry, rz, vx, vy, vz]
        # for each frame. 
        # More details: Unsupervised Learning of Depth and Ego-Motion from Video (CVPR 2017)
        if args.ego_type == "sfm":
            egomotion_path = os.path.join(data_dir, "egomotions/{}_egomotion.csv".format(video_id))
            egomotion_dict[video_id] = read_sfmlearn(egomotion_path, False)
        else:
            egomotion_path = os.path.join(data_dir, "egomotions/{}_gridflow_24.csv".format(video_id))
            egomotion_dict[video_id] = read_gridflow(egomotion_path, False)

        #Sorted sfm 6 features in frame orders and collects only these features.
        lr_mag_list = [abs(v[1]) for k, v in sorted(egomotion_dict[video_id].items())]

        start_dict[video_id] = {}
        traj_dict[video_id] = {}
        pose_dict[video_id] = {}

        # pid search
        pids = []
        for pid, info in trajectory_dict.items():

            if video_id in blacklist and pid in blacklist[video_id]:
                print("Blacklist: {} {}".format(video_id, pid))
                continue
            if "traj_sm" not in info:
                continue
            traj = info["traj_sm"]
            pose = info["pose_sm"]

            if len(traj) < args.traj_length:
                continue
            front_cnt = sum([1 if ps[11][0] - ps[8][0] > 0 else 0 for ps in pose])
            pids.append(pid)

        pid_cnt = len(pids)
        nb_trajs += pid_cnt

        traj_cnt = 0
        for pid in pids:
            info = trajectory_dict[pid]
            t_s = info["start"]
            traj = info["traj_sm"]
            pose = info["pose_sm"]
            pid = int(pid)

            if t_s <= 2 or t_s + len(traj) >= nb_images - 1:
                continue

            start_dict[video_id][pid] = info["start"]
            traj_dict[video_id][pid] = info["traj_sm"]
            pose_dict[video_id][pid] = info["pose_sm"]

            def add_sample(split):
                x_max = np.max([x[0] for x in traj[tidx+args.traj_length//2:tidx+args.traj_length]])
                x_min = np.min([x[0] for x in traj[tidx+args.traj_length//2:tidx+args.traj_length]])
                y_max = np.max([x[1] for x in traj[tidx+args.traj_length//2:tidx+args.traj_length]])
                y_min = np.min([x[1] for x in traj[tidx+args.traj_length//2:tidx+args.traj_length]])
                trans_mag = np.sqrt((x_max - x_min) ** 2 + (y_max - y_min) ** 2)
                turn_mag = np.max(lr_mag_list[tidx + t_s - 1 + args.traj_length // 2:tidx + t_s - 1 + args.traj_length])

                frames.append(tidx + t_s)
                person_ids.append(pid)
                trajectories.append(traj[tidx:tidx+args.traj_length])
                poses.append(pose[tidx:tidx+args.traj_length])
                splits.append(split)
                video_ids.append(video_id)

                turn_mags.append(turn_mag)
                trans_mags.append(trans_mag)

            # Training set (split 0-4)
            for tidx in range(0, len(traj) - args.traj_length + 1, args.traj_skip):
                add_sample(split_dict[video_id])
                traj_cnt += 1

            # Evaluation set (split 5-9)
            for tidx in range(0, len(traj) - args.traj_length + 1, args.traj_skip_test):
                add_sample(split_dict[video_id] + args.nb_splits)
                
        print("video id, num_images, num_pids, num_trajectories:")
        print(video_id, nb_images, pid_cnt, traj_cnt)
        nb_traj_list[split_dict[video_id]] += traj_cnt

    if not os.path.exists(os.path.dirname(out_fn)):
        os.makedirs(os.path.dirname(out_fn))
    
    print("splits =", splits)
    splits = np.array(splits)
    print("Total number of frames: {}".format(total_frames))
    result_str = ""
    for sp in range(args.nb_splits):
        result_str += "Split {}: {}".format(sp + 1, sum(splits == sp))
        if sp < args.nb_splits - 1:
            result_str += ", "

    print(result_str)
    print("Number of tracklets:", nb_trajs)
    print("Number of samples:", nb_traj_list)
    if not args.debug:
        joblib.dump({
            "video_ids": np.array(video_ids),
            "frames": np.array(frames),
            "person_ids": np.array(person_ids),
            "trajectories": np.array(trajectories),
            "poses": np.array(poses),
            "splits": splits,
            "egomotion_dict": egomotion_dict,
            "turn_mags": np.array(turn_mags),
            "trans_mags": np.array(trans_mags),
            "start_dict": start_dict,
            "traj_dict": traj_dict,
            "pose_dict": pose_dict
        }, out_fn)

        print(np.array(poses).shape)
        print("Written to {}".format(out_fn))
    print("Completed. Elapsed time: {} (s)".format(time.time()-start))
